<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Public opinion on AI governance | Artificial Intelligence: American Attitudes and Trends</title>
  <meta name="description" content="3 Public opinion on AI governance | Artificial Intelligence: American Attitudes and Trends" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Public opinion on AI governance | Artificial Intelligence: American Attitudes and Trends" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Public opinion on AI governance | Artificial Intelligence: American Attitudes and Trends" />
  
  
  

<meta name="author" content="Baobao Zhang and Allan Dafoe" />
<meta name="author" content="Center for the Governance of AI, Future of Humanity Institute, University of Oxford" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="general-attitudes-toward-ai.html">
<link rel="next" href="ai-policy-and-u-s-china-relations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-132060565-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-132060565-1');
</script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><img src="images/small_logo.png" alt="Report small logo" width="272px" hspace="12" vspace="12"/></li>
<li><a href="index.html"><b>Table of Contents</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="executive-summary.html"><a href="executive-summary.html"><i class="fa fa-check"></i><b>1</b> Executive summary</a><ul>
<li class="chapter" data-level="1.1" data-path="executive-summary.html"><a href="executive-summary.html#select-results"><i class="fa fa-check"></i><b>1.1</b> Select results</a></li>
<li class="chapter" data-level="1.2" data-path="executive-summary.html"><a href="executive-summary.html#reading-notes"><i class="fa fa-check"></i><b>1.2</b> Reading notes</a></li>
<li class="chapter" data-level="1.3" data-path="executive-summary.html"><a href="executive-summary.html#press-coverage"><i class="fa fa-check"></i><b>1.3</b> Press coverage</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html"><i class="fa fa-check"></i><b>2</b> General attitudes toward AI</a><ul>
<li class="chapter" data-level="2.1" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#subsecsupportai"><i class="fa fa-check"></i><b>2.1</b> More Americans support than oppose developing AI</a></li>
<li class="chapter" data-level="2.2" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#subsecdemosupportai"><i class="fa fa-check"></i><b>2.2</b> Support for developing AI is greater among those who are wealthy, educated, male, or have experience with technology</a></li>
<li class="chapter" data-level="2.3" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#subsecsupportmanageai"><i class="fa fa-check"></i><b>2.3</b> An overwhelming majority of Americans think that AI and robots should be carefully managed</a></li>
<li class="chapter" data-level="2.4" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#harmful-consequences-of-ai-in-the-context-of-other-global-risks"><i class="fa fa-check"></i><b>2.4</b> Harmful consequences of AI in the context of other global risks</a></li>
<li class="chapter" data-level="2.5" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#americans-understanding-of-key-technology-terms"><i class="fa fa-check"></i><b>2.5</b> Americans’ understanding of key technology terms</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html"><i class="fa fa-check"></i><b>3</b> Public opinion on AI governance</a><ul>
<li class="chapter" data-level="3.1" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#subsecgovchallenges13"><i class="fa fa-check"></i><b>3.1</b> Americans consider many AI governance challenges to be important; prioritize data privacy and preventing AI-enhanced cyber attacks, surveillance, and digital manipulation</a></li>
<li class="chapter" data-level="3.2" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#americans-who-are-younger-who-have-cs-or-engineering-degrees-express-less-concern-about-ai-governance-challenges"><i class="fa fa-check"></i><b>3.2</b> Americans who are younger, who have CS or engineering degrees express less concern about AI governance challenges</a></li>
<li class="chapter" data-level="3.3" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#americans-place-the-most-trust-in-the-u.s.-military-and-universities-to-build-ai-trust-tech-companies-and-non-governmental-organizations-more-than-the-government-to-manage-the-technology"><i class="fa fa-check"></i><b>3.3</b> Americans place the most trust in the U.S. military and universities to build AI; trust tech companies and non-governmental organizations more than the government to manage the technology</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html"><i class="fa fa-check"></i><b>4</b> AI policy and U.S.-China relations</a><ul>
<li class="chapter" data-level="4.1" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#americans-underestimate-the-u.s.-and-chinas-ai-research-and-development"><i class="fa fa-check"></i><b>4.1</b> Americans underestimate the U.S. and China’s AI research and development</a></li>
<li class="chapter" data-level="4.2" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#subsecexperimentchina"><i class="fa fa-check"></i><b>4.2</b> Communicating the dangers of a U.S.-China arms race requires explaining policy trade-offs</a></li>
<li class="chapter" data-level="4.3" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#americans-see-the-potential-for-u.s.-china-cooperation-on-some-ai-governance-challenges"><i class="fa fa-check"></i><b>4.3</b> Americans see the potential for U.S.-China cooperation on some AI governance challenges</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html"><i class="fa fa-check"></i><b>5</b> Trend across time: attitudes toward workplace automation</a><ul>
<li class="chapter" data-level="5.1" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html#americans-do-not-think-that-labor-market-disruptions-will-increase-with-time"><i class="fa fa-check"></i><b>5.1</b> Americans do not think that labor market disruptions will increase with time</a></li>
<li class="chapter" data-level="5.2" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html#extending-the-historical-time-trend"><i class="fa fa-check"></i><b>5.2</b> Extending the historical time trend</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html"><i class="fa fa-check"></i><b>6</b> High-level machine intelligence</a><ul>
<li class="chapter" data-level="6.1" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#arrivesooner"><i class="fa fa-check"></i><b>6.1</b> The public predicts a 54% likelihood of high-level machine intelligence within 10 years</a></li>
<li class="chapter" data-level="6.2" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#subsecsupporthlmi"><i class="fa fa-check"></i><b>6.2</b> Americans express mixed support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="6.3" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#subsecdemohlmi"><i class="fa fa-check"></i><b>6.3</b> High-income Americans, men, and those with tech experience express greater support for high-level machine intelligence</a></li>
<li class="chapter" data-level="6.4" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#subsecharmgood"><i class="fa fa-check"></i><b>6.4</b> The public expects high-level machine intelligence to be more harmful than good</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="appmethod.html"><a href="appmethod.html"><i class="fa fa-check"></i><b>A</b> Appendix A: Methodology</a><ul>
<li class="chapter" data-level="A.1" data-path="appmethod.html"><a href="appmethod.html#yougovsampling"><i class="fa fa-check"></i><b>A.1</b> YouGov sampling and weights</a></li>
<li class="chapter" data-level="A.2" data-path="appmethod.html"><a href="appmethod.html#appdemosubgroups"><i class="fa fa-check"></i><b>A.2</b> Demographic subgroups</a></li>
<li class="chapter" data-level="A.3" data-path="appmethod.html"><a href="appmethod.html#appanalysis"><i class="fa fa-check"></i><b>A.3</b> Analysis</a></li>
<li class="chapter" data-level="A.4" data-path="appmethod.html"><a href="appmethod.html#datasharing"><i class="fa fa-check"></i><b>A.4</b> Data sharing</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="apptopline.html"><a href="apptopline.html"><i class="fa fa-check"></i><b>B</b> Appendix B: Topline questionnaire</a><ul>
<li class="chapter" data-level="B.1" data-path="apptopline.html"><a href="apptopline.html#global_risks"><i class="fa fa-check"></i><b>B.1</b> Global risks</a></li>
<li class="chapter" data-level="B.2" data-path="apptopline.html"><a href="apptopline.html#considersai"><i class="fa fa-check"></i><b>B.2</b> Survey experiment: what the public considers AI, automation, machine learning, and robotics</a></li>
<li class="chapter" data-level="B.3" data-path="apptopline.html"><a href="apptopline.html#knowledge-of-computer-science-cstechnology"><i class="fa fa-check"></i><b>B.3</b> Knowledge of computer science (CS)/technology</a></li>
<li class="chapter" data-level="B.4" data-path="apptopline.html"><a href="apptopline.html#supportdevai"><i class="fa fa-check"></i><b>B.4</b> Support for developing AI</a></li>
<li class="chapter" data-level="B.5" data-path="apptopline.html"><a href="apptopline.html#manageexp"><i class="fa fa-check"></i><b>B.5</b> Survey experiment: AI and/or robots should be carefully managed</a></li>
<li class="chapter" data-level="B.6" data-path="apptopline.html"><a href="apptopline.html#trustdevai"><i class="fa fa-check"></i><b>B.6</b> Trust of actors to develop AI</a></li>
<li class="chapter" data-level="B.7" data-path="apptopline.html"><a href="apptopline.html#trustmanageai"><i class="fa fa-check"></i><b>B.7</b> Trust of actors to manage AI</a></li>
<li class="chapter" data-level="B.8" data-path="apptopline.html"><a href="apptopline.html#govchallenges"><i class="fa fa-check"></i><b>B.8</b> AI governance challenges</a></li>
<li class="chapter" data-level="B.9" data-path="apptopline.html"><a href="apptopline.html#airesearchcompare"><i class="fa fa-check"></i><b>B.9</b> Survey experiment: comparing perceptions of U.S. vs. China AI research and development</a></li>
<li class="chapter" data-level="B.10" data-path="apptopline.html"><a href="apptopline.html#armsraceexp"><i class="fa fa-check"></i><b>B.10</b> Survey experiment: U.S.-China arms race</a><ul>
<li class="chapter" data-level="B.10.1" data-path="apptopline.html"><a href="apptopline.html#control"><i class="fa fa-check"></i><b>B.10.1</b> Control</a></li>
<li class="chapter" data-level="B.10.2" data-path="apptopline.html"><a href="apptopline.html#nationalism-treatment"><i class="fa fa-check"></i><b>B.10.2</b> Nationalism treatment</a></li>
<li class="chapter" data-level="B.10.3" data-path="apptopline.html"><a href="apptopline.html#war-risks-treatment"><i class="fa fa-check"></i><b>B.10.3</b> War risks treatment</a></li>
<li class="chapter" data-level="B.10.4" data-path="apptopline.html"><a href="apptopline.html#common-humanity-treatment"><i class="fa fa-check"></i><b>B.10.4</b> Common humanity treatment</a></li>
</ul></li>
<li class="chapter" data-level="B.11" data-path="apptopline.html"><a href="apptopline.html#uschinacoop"><i class="fa fa-check"></i><b>B.11</b> Issue areas for possible U.S.-China cooperation</a></li>
<li class="chapter" data-level="B.12" data-path="apptopline.html"><a href="apptopline.html#jobtime"><i class="fa fa-check"></i><b>B.12</b> Trend across time: job creation or job loss</a></li>
<li class="chapter" data-level="B.13" data-path="apptopline.html"><a href="apptopline.html#forecasthlmi"><i class="fa fa-check"></i><b>B.13</b> High-level machine intelligence: forecasting timeline</a></li>
<li class="chapter" data-level="B.14" data-path="apptopline.html"><a href="apptopline.html#supporthlmi"><i class="fa fa-check"></i><b>B.14</b> Support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="B.15" data-path="apptopline.html"><a href="apptopline.html#expectedoutcome"><i class="fa fa-check"></i><b>B.15</b> Expected outcome of high-level machine intelligence</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="addresults.html"><a href="addresults.html"><i class="fa fa-check"></i><b>C</b> Appendix C: Additional data analysis results</a><ul>
<li class="chapter" data-level="C.1" data-path="addresults.html"><a href="addresults.html#addsupportdevai"><i class="fa fa-check"></i><b>C.1</b> Support for developing AI</a></li>
<li class="chapter" data-level="C.2" data-path="addresults.html"><a href="addresults.html#addcarefullym"><i class="fa fa-check"></i><b>C.2</b> Survey experiment and cross-national comparison: AI and/or robots should be carefully managed</a></li>
<li class="chapter" data-level="C.3" data-path="addresults.html"><a href="addresults.html#appglobalrisks"><i class="fa fa-check"></i><b>C.3</b> Harmful consequences of AI in the context of other global risks</a></li>
<li class="chapter" data-level="C.4" data-path="addresults.html"><a href="addresults.html#aawhatsai"><i class="fa fa-check"></i><b>C.4</b> Survey experiment: what the public considers AI, automation, machine learning, and robotics</a></li>
<li class="chapter" data-level="C.5" data-path="addresults.html"><a href="addresults.html#appgovchallenges"><i class="fa fa-check"></i><b>C.5</b> AI governance challenges: prioritizing governance challenges</a></li>
<li class="chapter" data-level="C.6" data-path="addresults.html"><a href="addresults.html#trust-in-various-actors-to-develop-and-manage-ai-in-the-interest-of-the-public"><i class="fa fa-check"></i><b>C.6</b> Trust in various actors to develop and manage AI in the interest of the public</a></li>
<li class="chapter" data-level="C.7" data-path="addresults.html"><a href="addresults.html#appuschinacomp"><i class="fa fa-check"></i><b>C.7</b> Survey experiment: comparing perceptions of U.S. vs. China AI research and development</a></li>
<li class="chapter" data-level="C.8" data-path="addresults.html"><a href="addresults.html#appuschinaarmsrace"><i class="fa fa-check"></i><b>C.8</b> Survey experiment: U.S.-China arms race</a></li>
<li class="chapter" data-level="C.9" data-path="addresults.html"><a href="addresults.html#appjobloss"><i class="fa fa-check"></i><b>C.9</b> Trend across time: job creation or job loss</a></li>
<li class="chapter" data-level="C.10" data-path="addresults.html"><a href="addresults.html#apphlmi"><i class="fa fa-check"></i><b>C.10</b> High-level machine intelligence: forecasting timeline</a></li>
<li class="chapter" data-level="C.11" data-path="addresults.html"><a href="addresults.html#appsupporthlmi"><i class="fa fa-check"></i><b>C.11</b> Support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="C.12" data-path="addresults.html"><a href="addresults.html#appexpectedoutcome"><i class="fa fa-check"></i><b>C.12</b> Expected outcome of high-level machine intelligence</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a><ul>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#primary-researchers"><i class="fa fa-check"></i>Primary researchers</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#editing-and-design"><i class="fa fa-check"></i>Editing and design</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#funders"><i class="fa fa-check"></i>Funders</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#for-media-or-other-inquiries"><i class="fa fa-check"></i>For media or other inquiries</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#recommended-citation"><i class="fa fa-check"></i>Recommended citation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html"><i class="fa fa-check"></i>About us</a><ul>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#about-the-center-for-the-governance-of-ai"><i class="fa fa-check"></i>About the Center for the Governance of AI</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#about-the-future-of-humanity-institute"><i class="fa fa-check"></i>About the Future of Humanity Institute</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://governance.ai">Center for the Governance of AI</a></li>
<li><a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a></li>
<li><a href="http://www.ox.ac.uk/">University of Oxford</a></li>
<li><img src="images/FHI-Logo-Print.png" alt="FHI logo" width="77px" hspace="12"/><img src="images/oxford-university-logo.png" alt="Oxford logo" width="74px" hspace="12"/></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Artificial Intelligence: American Attitudes and Trends</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="public-opinion-on-ai-governance" class="section level1">
<h1><span class="header-section-number">3</span> Public opinion on AI governance</h1>
<div id="subsecgovchallenges13" class="section level2">
<h2><span class="header-section-number">3.1</span> Americans consider many AI governance challenges to be important; prioritize data privacy and preventing AI-enhanced cyber attacks, surveillance, and digital manipulation</h2>
<!-- 
BZ: I think this section is solid -- maybe 95% there. The literature review section could be cut because I'm not sure it's 100% relevant to the results.
AD: "over solid"? What does that mean? 
[BZ: I made a typo]

AD: why do we call this "attitudes", vs "beliefs"? Just wondering. Seems like "beliefs" is more respectful.
[BZ: I changed it to "public opinion".]
-->
<p>We sought to understand how Americans prioritize policy issues associated with AI. Respondents were asked to consider five AI governance challenges, randomly selected from a test of 13 (<a href="apptopline.html#govchallenges">see Appendix B for the text</a>); the order these five were to each respondent was also randomized.</p>
<!-- 
AD to BZ: See what I did above with the hyperlink? I think it would be good to do that for most hyperlinks, so reader knows what the link is to, and so print copy makes sense as well. 
BZ_2911: I will fix the hyperlinks. 
-->
<p>After considering each governance challenge, respondents were asked how likely they think the challenge will affect large numbers of people 1) in the U.S. and 2) around the world within 10 years.</p>
<p>We use scatterplots to visualize our survey results. In Figure <a href="public-opinion-on-ai-governance.html#fig:airisksus">3.1</a>, the <em>x</em>-axis is the perceived likelihood of the problem happening to large numbers of people in the U.S. In Figure <a href="public-opinion-on-ai-governance.html#fig:airisksworld">3.2</a>, the <em>x</em>-axis is the perceived likelihood of the problem happening to large numbers of people around the world. The <em>y</em>-axes on both Figure <a href="public-opinion-on-ai-governance.html#fig:airisksus">3.1</a> and <a href="public-opinion-on-ai-governance.html#fig:airisksworld">3.2</a> represent respondents’ perceived issue importance, from 0 (not at all important) to 3 (very important). Each dot represents the mean perceived likelihood and issue importance, and the correspondent ellipse represents the 95% bivariate confidence region.</p>
<p>Americans consider all the AI governance challenges we present to be important: the mean perceived issues importance of each governance challenge is between “somewhat important” (2) and “very important” (3), though there is meaningful and discernible variation across items.</p>
<p>The AI governance challenges Americans think are most likely to impact large numbers of people, and are important for tech companies and governments to tackle, are found in the upper-right quadrant of the two plots. These issues include data privacy as well as AI-enhanced cyber attacks, surveillance, and digital manipulation. We note that the media have widely covered these issues during the time of the survey.</p>
<p>There are a second set of governance challenges that are perceived on average, as about 7% less likely, and marginally less important. These include autonomous vehicles, value alignment, bias in using AI for hiring, the U.S.-China arms race, disease diagnosis, and technological unemployment. Finally, the third set of challenges are perceived on average another 5% less likely, and about equally important, including criminal justice bias and critical AI systems failures.</p>
<p>We also note that Americans predict that all of the governance challenges mentioned in the survey, besides protecting data privacy and ensuring the safety of autonomous vehicles, are more likely to impact people around the world than to affect people in the U.S. While most of the statistically significant differences are substantively small, one difference stands out: Americans think that autonomous weapons are 7.6 percentage points more likely to impact people around the world than Americans. (See <a href="addresults.html#appgovchallenges">Appendix C</a> for details of these additional analyses.)</p>
<p>We want to reflect on one result. “Value alignment” consists of an abstract description of alignment problem and a reference to what sounds like individual level harms: “while performing jobs [they could] unintentionally make decisions that go against the values of its human users, such as physically harming people.” “Critical AI systems failures,” by contrast, references military or critical infrastructure uses, and unintentional accidents leading to “10 percent or more of all humans to die.” The latter was weighted as less important than the former: we interpret this as a probability weighted assessment of importance, since presumably the latter, were it to happen, is much more important. We thus think the issue importance question should be interpreted in a way that down-weights low probability risks. This perspective also plausibly applies to the “impact” measure for our global risks analysis, which placed “harmful consequences of synthetic biology” and “failure to address climate change” as less impactful than most other risks.</p>
<div class="figure"><span id="fig:airisksus"></span>
<img src="ai_public_opinion_us_2018_report-190107_web_files/figure-html/airisksus-1.png" alt="Perceptions of AI governance challenges in the U.S." width="2700" />
<p class="caption">
Figure 3.1: Perceptions of AI governance challenges in the U.S.
</p>
</div>
<div class="figure"><span id="fig:airisksworld"></span>
<img src="ai_public_opinion_us_2018_report-190107_web_files/figure-html/airisksworld-1.png" alt="Perceptions of AI governance challenges around the world" width="2700" />
<p class="caption">
Figure 3.2: Perceptions of AI governance challenges around the world
</p>
</div>
</div>
<div id="americans-who-are-younger-who-have-cs-or-engineering-degrees-express-less-concern-about-ai-governance-challenges" class="section level2">
<h2><span class="header-section-number">3.2</span> Americans who are younger, who have CS or engineering degrees express less concern about AI governance challenges</h2>
<p>We performed further analysis by calculating the percentage of respondents in each subgroup who consider each governance challenge to be “very important” for governments and tech companies to manage. (See <a href="addresults.html#appgovchallenges">Appendix C</a> for additional data visualizations.) In general, differences in responses are more salient across demographic subgroups than across governance challenges. In a linear multiple regression predicting perceived issue importance using demographic subgroups, governance challenges, and the interaction between the two, we find that the stronger predictors are demographic subgroup variables, including age group and having CS or programming experience.</p>
<p>Two highly visible patterns emerge from our data visualization. First, a higher percentage of older respondents, compared with younger respondents, consider nearly all AI governance challenges to be “very important.” As discussed previously, we find that older Americans, compared with younger Americans, are less supportive of developing AI. Our results here might explain this age gap: older Americans see each AI governance challenge as substantially more important than do younger Americans. Whereas 85% of Americans older than 73 consider each of these issues to be very important, only 40% of Americans younger than 38 do.</p>
<p>Second, those with CS or engineering degrees, compared with those who do not, rate all AI governance challenges as less important. This result could explain our previous finding that those with CS or engineering degrees tend to exhibit greater support for developing AI.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<div class="figure"><span id="fig:airisksdemo"></span>
<img src="ai_public_opinion_us_2018_report-190107_web_files/figure-html/airisksdemo-1.png" alt="AI governance challenges: issue importance by demographic subgroups" width="2100" />
<p class="caption">
Figure 3.3: AI governance challenges: issue importance by demographic subgroups
</p>
</div>
</div>
<div id="americans-place-the-most-trust-in-the-u.s.-military-and-universities-to-build-ai-trust-tech-companies-and-non-governmental-organizations-more-than-the-government-to-manage-the-technology" class="section level2">
<h2><span class="header-section-number">3.3</span> Americans place the most trust in the U.S. military and universities to build AI; trust tech companies and non-governmental organizations more than the government to manage the technology</h2>
<p>Respondents were asked how much confidence they have in various actors to develop AI. They were randomly assigned five actors out of 15 to evaluate. We provided a short description of actors that are not well-known to the public (e.g., NATO, CERN, and OpenAI).</p>
<p>Also, respondents were asked how much confidence, if any, they have in various actors to manage the development and use of AI in the best interests of the public. They were randomly assigned five out of 15 actors to evaluate. Again, we provided a short description of actors that are not well-known to the public (e.g., AAAI and Partnership on AI). Confidence was measured using the same four-point scale described above.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<p>Americans do not express great confidence in most actors to develop or to manage AI, as reported in Figures <a href="public-opinion-on-ai-governance.html#fig:trustdevtable1">3.4</a> and <a href="public-opinion-on-ai-governance.html#fig:trustdevtable2">3.5</a>. A majority of Americans do not have a “great deal” or even a “fair amount” of confidence in any institution, except university researchers, to develop AI. Furthermore, Americans place greater trust in tech companies and non-governmental organizations (e.g., OpenAI) than in governments to manage the development and use of the technology.</p>
<p>University researchers and the U.S. military are the most trusted groups to develop AI: about half of Americans express a “great deal” or even a “fair amount” of confidence in them. Americans express slightly less confidence in tech companies, non-profit organizations (e.g., OpenAI), and American intelligence organizations. Nevertheless, opinions toward individual actors within each of these groups vary. For example, while 44% of Americans indicated they feel a “great deal” or even a “fair amount” of confidence in tech companies, they rate Facebook as the least trustworthy of all the actors. More than four in 10 indicate that they have no confidence in the company.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<p>The results on the public’s trust of various actors to manage the develop and use of AI provided are similar to the results discussed above. Again, a majority of Americans do not have a “great deal” or even a “fair amount” of confidence in any institution to manage AI. In general, the public expresses greater confidence in non-governmental organizations than in governmental ones. Indeed, 41% of Americans express a “great deal” or even a “fair amount” of confidence in “tech companies,” compared with 26% who feel that way about the U.S. federal government. But when presented with individual big tech companies, Americans indicate less trust in each than in the broader category of “tech companies.” Once again, Facebook stands out as an outlier: respondents give it a much lower rating than any other actor. Besides “tech companies,” the public places relatively high trust in intergovernmental research organizations (e.g., CERN), the Partnership on AI, and non-governmental scientific organizations (e.g., AAAI). Nevertheless, because the public is less familiar with these organizations, about one in five respondents give a “don’t know” response.</p>
<p>Mirroring our findings, recent survey research suggests that while Americans feel that AI should be regulated, they are unsure <em>who</em> the regulators should be. When asked who “should decide how AI systems are designed and deployed,” half of Americans indicated they do not know or refused to answer <span class="citation">(West <a href="#ref-west2018divided">2018</a><a href="#ref-west2018divided">a</a>)</span>. Our survey results seem to reflect Americans’ general attitudes toward public institutions. According to a 2016 Pew Research Center survey, an overwhelming majority of Americans have “a great deal” or “a fair amount” of confidence in the U.S. military and scientists to act in the best interest of the public. In contrast, public confidence in elected officials is much lower: 73% indicated that they have “not too much” or “no confidence” <span class="citation">(Funk <a href="#ref-funk2017">2017</a>)</span>. Less than one-third of Americans thought that tech companies do what’s right “most of the time” or “just about always”; moreover, more than half think that tech companies have too much power and influence in the U.S. economy <span class="citation">(Smith <a href="#ref-smith2018">2018</a>)</span>. Nevertheless, Americans’ attitude toward tech companies is not monolithic but varies by company. For instance, our research findings reflect the results from a 2018 survey, which reported that a higher percentage of Americans trusted Apple, Google, Amazon, Microsoft, and Yahoo to protect user information than trust Facebook to do so <span class="citation">(Ipsos and Reuters <a href="#ref-ipsosreuters2018">2018</a>)</span>.</p>
<div class="figure"><span id="fig:trustdevtable1"></span>
<img src="ai_public_opinion_us_2018_report-190107_web_files/figure-html/trustdevtable1-1.png" alt="Trust in various actors to develop AI: distribution of responses" width="2100" />
<p class="caption">
Figure 3.4: Trust in various actors to develop AI: distribution of responses
</p>
</div>
<div class="figure"><span id="fig:trustdevtable2"></span>
<img src="ai_public_opinion_us_2018_report-190107_web_files/figure-html/trustdevtable2-1.png" alt="Trust in various actors to manage AI: distribution of responses" width="2100" />
<p class="caption">
Figure 3.5: Trust in various actors to manage AI: distribution of responses
</p>
</div>
<div class="figure"><span id="fig:trustcoef"></span>
<img src="ai_public_opinion_us_2018_report-190107_web_files/figure-html/trustcoef-1.png" alt="Trust in various actors to develop and manage AI in the interest of the public" width="1800" />
<p class="caption">
Figure 3.6: Trust in various actors to develop and manage AI in the interest of the public
</p>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-funk2017">
<p>Funk, Cary. 2017. “Real Numbers: Mixed Messages About Public Trust in Science.” <em>Issues in Science and Technology</em> 34 (1). <a href="https://perma.cc/UF9P-WSRL">https://perma.cc/UF9P-WSRL</a>.</p>
</div>
<div id="ref-ipsosreuters2018">
<p>Ipsos and Reuters. 2018. “Facebook Poll 3.23.18.” Survey report. Ipsos; Reuters. <a href="https://perma.cc/QGH5-S3KE">https://perma.cc/QGH5-S3KE</a>.</p>
</div>
<div id="ref-smith2018">
<p>Smith, Aaron. 2018. “Public Attitudes Toward Technology Companies.” Survey report. Pew Research Center. <a href="https://perma.cc/KSN6-6FRW">https://perma.cc/KSN6-6FRW</a>.</p>
</div>
<div id="ref-west2018divided">
<p>West, Darrell M. 2018a. “Brookings Survey Finds Divided Views on Artificial Intelligence for Warfare, but Support Rises If Adversaries Are Developing It.” Survey report. Brookings Institution. <a href="https://perma.cc/3NJV-5GV4">https://perma.cc/3NJV-5GV4</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>In Table <a href="addresults.html#tab:aigovregsat">C.15</a>, we report the results of a saturated linear model using demographic variables, governance challenges, and the interaction between these two types of variables to predict perceived issue importance. We find that those who are 54-72 or 73 and older, relative to those who are below 38, view the governance issues as more important (two-sided <span class="math inline">\(p\)</span>-value &lt; 0.001 for both comparisons). Furthermore, we find that those who have CS or engineering degrees, relative to those who do not, view the governance challenges as less important (two-sided <span class="math inline">\(p\)</span>-value &lt; 0.001).<a href="public-opinion-on-ai-governance.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>The two sets of 15 actors differed slightly because for some actors it seemed inappropriate to ask one or the other question. See <a href="apptopline.html#trustdevai">Appendix B</a> for the exact wording of the questions and descriptions of the actors.<a href="public-opinion-on-ai-governance.html#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>Our survey was conducted between June 6 and 14, 2018, shortly after the fallout of the Facebook/Cambridge Analytica scandal. On April 10-11, 2018, Facebook CEO Mark Zuckerberg testified before the U.S. Congress regarding the Cambridge Analytica data leak. On May 2, 2018, Cambridge Analytica announced its shutdown. Nevertheless, Americans’ distrust of the company existed before the Facebook/Cambridge Analytica scandal. In a pilot survey that we conducted on Mechanical Turk during July 13-14, 2017, respondents indicated a substantially lower level of confidence in Facebook, compared with other actors, to develop and regulate AI.<a href="public-opinion-on-ai-governance.html#fnref10" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="general-attitudes-toward-ai.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ai-policy-and-u-s-china-relations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": [["https://governanceai.github.io/US-Public-Opinion-Report-Jan-2019/us_public_opinion_report_jan_2019.pdf", "PDF"], ["https://ssrn.com/abstract=3312874", "SSRN"], ["https://doi.org/10.7910/DVN/SGFRYA", "Replication Data"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
